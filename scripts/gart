#!/bin/bash

KAFKA_HOME=$KAFKA_HOME
MAXWELL_HOME=$MAXWELL_HOME

####################### Parse Arguments #######################
VALID_ARGS=$(getopt -o d:b:u:p:r:t:e:k:h? --long db-host:,db-name:,db-port:,user:,password:,rgmapping-file:,table-schema-file:,v6d-sock:,v6d-size:,use-debezium:,db-type:,enable-bulkload:,etcd-endpoint:,etcd-prefix:,kafka-server:,subgraph-num:,hostfile:,help -- "$@")
if [[ $? -ne 0 ]]; then
    exit 1;
fi

db_host="127.0.0.1"
db_port=3306
db_name="ldbc"
user=
password=
rgmapping_file="schema/rgmapping-ldbc.yaml"
table_schema_file="schema/db_schema.json"
v6d_sock="/var/run/vineyard.sock"
v6d_size="750G"
etcd_endpoint="http://127.0.0.1:2379"
etcd_prefix="gart_meta_"
kafka_server="127.0.0.1:9092"
subgraph_num=1
use_debezium=1
db_type="postgresql"
enable_bulkload=false
hostfile=

set_port=false
eval set -- "$VALID_ARGS"
while [ : ]; do
  case "$1" in
    -d | --db-host)
        db_host=$2
        shift 2
        ;;
         --db-port)
        db_port=$2
        shift 2
        set_port=true
        ;;
    -b | --db-name)
        db_name=$2
        shift 2
        ;;
    -u | --user)
        user=$2
        shift 2
        ;;
    -p | --password)
        password=$2
        shift 2
        ;;
    -r | --rgmapping-file)
        rgmapping_file=$2
        shift 2
        ;;
    -t | --table-schema-file)
        table_schema_file=$2
        shift 2
        ;;
         --v6d-sock)
        v6d_sock=$2
        shift 2
        ;;
         --v6d-size)
        v6d_size=$2
        shift 2
        ;;
        --use-debezium)
        use_debezium=$2
        shift 2
        ;;
         --db-type)
        db_type=$2
        shift 2
        ;;
        --enable-bulkload)
        enable_bulkload=$2
        shift 2
        ;;
    -e | --etcd-endpoint)
        etcd_endpoint=$2
        shift 2
        ;;
         --etcd-prefix)
        etcd_prefix=$2
        shift 2
        ;;
    -k | --kafka-server)
        kafka_server=$2
        shift 2
        ;;
         --subgraph-num)
        subgraph_num=$2
        shift 2
        ;;
         --hostfile)
        hostfile=$2
        shift 2
        ;;
    -h | --help | ?)
        echo "Usage: $0 [options]"
        echo "      --db-type:           type of data source: postgresql, mysql (default: postgresql)"
        echo "  -d, --db-host:           database host (default: 127.0.0.1)"
        echo "      --db-port:           database port (default: 3306 for MySQL, 5432 for PostgreSQL)"
        echo "  -b, --db-name:           database name (default: ldbc)"
        echo "  -u, --user:              database user"
        echo "  -p, --password:          database password"
        echo "  -r, --rgmapping-file:    rgmapping file path (default: schema/rgmapping-ldbc.yaml)"
        echo "  -t, --table-schema-file: table schema file path (default: schema/db_schema.json)"
        echo "      --v6d-sock:          vineyard socket path (default: /var/run/vineyard.sock)"
        echo "      --v6d-size:          vineyard size (default: 110G)"
        echo "      --use-debezium:      if use Debezium to parse txn logs (default: 1)"
        echo "      --enable-bulkload:   if enable bulkload (default: false)"
        echo "  -e, --etcd-endpoint:     etcd endpoint (default: http://127.0.0.1:2379)"
        echo "      --etcd-prefix:       etcd meta prefix (default: gart_meta_)"
        echo "  -k, --kafka-server:      kafka bootstrap server (default: 127.0.0.1:9092)"
        echo "      --subgraph-num:      number of sub graphs (default: 1)"
        echo "      --hostfile:          hostfile for distributed deployment"
        echo "  -h, --help:              help"
        exit 0
        ;;
    --)
        shift;
        break
        ;;
  esac
done

# check file path
function make_absolute_path() {
  local path=$1
  if [[ ! $path = /* ]]; then
    path=$(readlink -f "$path" 2>/dev/null || pwd -P)
  fi
  echo "$path"
}

v6d_sock=$(make_absolute_path "$v6d_sock")
rgmapping_file=$(make_absolute_path "$rgmapping_file")
table_schema_file=$(make_absolute_path "$table_schema_file")

# set default port
if [ !$set_port ] && [ "$db_type" == "postgresql" ]; then
    db_port=5432
fi

if [ "$enable_bulkload" == 1 ]; then
    enable_bulkload=true
fi

if [ "$enable_bulkload" == 0 ]; then
    enable_bulkload=false
fi

# check arguments
if [ "$db_type" != "mysql" ] && [ "$db_type" != "postgresql" ]; then
    echo "Invalid value for 'db_type': $db_type"
    exit 1
fi

if [ "$use_debezium" == 0 ] && [ "$db_type" != "mysql" ]; then
    echo "Maxwell only supports MySQL"
    exit 1
fi

# check environment variables
unset=false
if [ -n "$KAFKA_HOME" ]; then
    echo "    KAFKA_HOME: $KAFKA_HOME"
else
    echo "KAFKA_HOME is empty, please set the environment variable KAFKA_HOME"
    unset=true
fi

if [ "$use_debezium" == 0 ]; then
    if [ -n "$MAXWELL_HOME" ]; then
        echo "    MAXWELL_HOME: $MAXWELL_HOME"
    else
        echo "MAXWELL_HOME is empty, please set the environment variable MAXWELL_HOME"
        unset=true
    fi
fi

if [ "$unset" = true ]; then
    exit 1
fi

# check a host is local host or not
function is_local_host() {
    local_ips=$(hostname -I)

    for ip in ${local_ips[@]}; do
        if echo "$1" | grep -qE "(^| )($ip|localhost|127\.0\.0\.1)( |$)"; then
            echo "true"
            return
        fi
    done
    echo "false"
}

# check and parse the hostfile if exists
start_slot=0
hosts=()
is_master=true
slot=$subgraph_num
if [ -n "$hostfile" ]; then
    if [ ! -f "$hostfile" ]; then
        echo "Hostfile $hostfile does not exist"
        exit 1
    fi

    slot=0
    # check format
    line_number=0
    while read -r line; do
        line_number=$((line_number+1))

        if ! echo "$line" | grep -qE '^(\S+)\s+slots=[0-9]+$'; then
          echo "Hostfile format is invalid at line $line_number: $line"
          exit 1
        fi
    done < $hostfile

    # parse hostfile
    sum_cores=0
    while read -r line; do
        hostname=$(echo "$line" | awk '{print $1}')
        slot=$(echo "$line" | awk '{print substr($2, 7)}') # remove "slots="

        tuple=("${hostname}:${slot}")
        hosts+=("${tuple[@]}")
        sum_cores=$((sum_cores+slot))
    done < $hostfile

    if [ $sum_cores -ne $subgraph_num ]; then
        echo "The number of slots in hostfile is not equal to subgraph number"
        exit 1
    fi
fi

i=0
found_mac=false
master_hostname="127.0.0.1"
for item in ${hosts[@]}; do
    for pair in $item; do
        hostname=$(echo $pair | cut -d ':' -f1)
        slot=$(echo $pair | cut -d ':' -f2)
        #echo "Host: $hostname, Slot: $slot"
        if [ $i -eq 0 ]; then
            master_hostname=$hostname
        fi

        if [ $(is_local_host "$hostname") == "true" ]; then
            #echo "Start to launch subgraph $i on $hostname"
            if [ $i -ne 0 ]; then
                is_master=false
            fi
            found_mac=true
        else
            if [ $found_mac == false ]; then
                start_slot=$((start_slot+slot))
            fi
        fi
    done
    i=$((i+1))
done

echo "whoami: $(whoami)"
echo "Arguments:"
echo "    db-type: $db_type"
echo "    db-host: $db_host"
echo "    db-port: $db_port"
echo "    db-name: $db_name"
echo "    user: $user"
echo "    password: $password"
echo "    rgmapping-file: $rgmapping_file"
echo "    table-schema-file: $table_schema_file"
echo "    v6d-sock: $v6d_sock"
echo "    v6d-size: $v6d_size"
echo "    use-debezium: $use_debezium"
echo "    enable-bulkload: $enable_bulkload"
echo "    etcd-endpoint: $etcd_endpoint"
echo "    etcd-prefix: $etcd_prefix"
echo "    kafka-server: $kafka_server"
echo "    subgraph-num: $subgraph_num"
echo "    hostfile: $hostfile"

echo "    is-master: $is_master"
echo "    start-slot: $start_slot"
echo "    my-slot: $slot"
echo "    master-hostname: $master_hostname"

##################### Stop Previous GART ######################
echo "Stop previous GART"
./stop-gart --kill-v6d-sock $v6d_sock

####################### Clean Etcd #######################
echo "Clean etcd"
etcdctl --endpoints=$etcd_endpoint del --prefix $etcd_prefix > /dev/null 2>&1

####################### Extract Schema #######################
if [ $is_master == true ]; then
    echo "Start extracting schema"
    num_row=$(
    ./scripts/extract_table_schema.py \
        --host $db_host --port $db_port --db $db_name \
        --user $user --password $password --db_type $db_type\
        --rgmapping_file $rgmapping_file --output $table_schema_file
    )
    if [ $num_row -eq 0 ] && $enable_bulkload; then
        echo "No tuples found in the database, skip bulk load"
        enable_bulkload=false
    fi
fi

declare -A pids     # array to store pids and comments

################### Bulk Load Initialize ######################
if [ "$use_debezium" == 1 ] && $enable_bulkload; then
    rm /tmp/connect.offsets 1 > /dev/null 2>&1
    echo "Remove /tmp/connect.offsets for bulk load"
fi

####################### Vineyard #######################
if [ ! -e "$v6d_sock" ] || [ ! -S "$v6d_sock" ]; then
    echo "Start to launch vineyardd on $v6d_sock"
    # TODO: etcd_cmd user for PostgreSQL Extension
    vineyardd --etcd_cmd=/usr/local/bin/etcd --socket $v6d_sock --size $v6d_size --etcd_endpoint $etcd_endpoint  --norpc > /dev/null 2>&1 &
    pids["Vineyard"]=$!
fi

####################### Capturer: Zookeeper and Kafka #######################
if [ $is_master == true ]; then
    KAFKA_BIN=$KAFKA_HOME/bin
    KAFKA_CONFIG=$KAFKA_HOME/config
    KAFKA_LOG=$KAFKA_HOME/logs/kafkaServer.out

    echo "Start Zookeeper and Kafka"
    $KAFKA_BIN/zookeeper-server-start.sh -daemon $KAFKA_CONFIG/zookeeper.properties
    # pids["Zookeeper"]=$!  # TODO(SSJ): check if Zookeeper is started
    sleep 3

    $KAFKA_BIN/kafka-server-start.sh -daemon $KAFKA_CONFIG/server.properties
    # pids["Kafka"]=$!  # TODO(SSJ): check if Kafka is started
    sleep 5

    # check Kafka failure
    for i in {1..3}
    do
        if grep -q "ERROR" "$KAFKA_LOG"; then
            if [ $i -ne 1 ]; then
                echo "File $KAFKA_LOG contains ERROR, waiting for cleanning"
                sleep 10
            fi

            $KAFKA_BIN/zookeeper-server-start.sh -daemon $KAFKA_CONFIG/zookeeper.properties
            #pids["Zookeeper"]=$!
            sleep 3

            $KAFKA_BIN/kafka-server-start.sh -daemon $KAFKA_CONFIG/server.properties
            #pids["Kafka"]=$!
            sleep 5
        else
            break
        fi
        if [ $i -eq 3 ]; then
            echo "Kafka: Reached max attempts, ERROR found"
            ./stop-gart --kill-v6d-sock $v6d_sock
            exit 1
        fi
    done

    echo "    Clean topics: binlog & unified_log"
    $KAFKA_BIN/kafka-topics.sh --delete --topic 'binlog.*' --bootstrap-server 127.0.0.1:9092 > /dev/null 2>&1
    $KAFKA_BIN/kafka-topics.sh --delete --topic unified_log --bootstrap-server 127.0.0.1:9092 > /dev/null 2>&1

    sleep 2

    #TODO: hard code: replication-factor

    echo "    Create topic: binlog"
    $KAFKA_BIN/kafka-topics.sh --create --topic binlog --bootstrap-server 127.0.0.1:9092 --partitions 1 --replication-factor 1 > /dev/null 2>&1

    echo "    Create topic: unified_log"
    $KAFKA_BIN/kafka-topics.sh --create --topic unified_log --bootstrap-server 127.0.0.1:9092 --partitions 1 --replication-factor 1 > /dev/null 2>&1

    # $KAFKA_BIN/kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list
fi


####################### Capturer: Maxwell #######################
if [ "$use_debezium" == 0 ]; then
    if [ $is_master == true ]; then
        MAXWELL_BIN=$MAXWELL_HOME/bin

        echo "Start Maxwell"
        $MAXWELL_BIN/maxwell --host=$db_host --user=$user --password=$password --producer=kafka --kafka.bootstrap.servers=127.0.0.1:9092 --kafka_topic=binlog > /dev/null 2>&1 &

        pids["Maxwell"]=$!
        sleep 3
    fi
fi

####################### Capturer: Debezium #######################
# TODO(wanglei): add distributed deployment of debezium
if [ "$use_debezium" == 1 ]; then
    if [ $is_master == true ]; then
        unset CLASSPATH
        echo "Start Debezium"
        if [ "$db_type" == "mysql" ]; then
            echo "    Start Debezium connector for MySQL"
            $KAFKA_BIN/connect-standalone.sh $KAFKA_CONFIG/connect-standalone.properties $KAFKA_CONFIG/connect-debezium-mysql.properties > /dev/null 2>&1 &
        elif [ "$db_type" == "postgresql" ]; then
            echo "    Start Debezium connector for PostgreSQL"
            $KAFKA_BIN/connect-standalone.sh $KAFKA_CONFIG/connect-standalone.properties $KAFKA_CONFIG/connect-debezium-postgresql.properties > /dev/null 2>&1 &
        fi
        pids["Debezium"]=$!
        sleep 5
    fi
fi

####################### Converter #######################
if [ $is_master == true ]; then
    CONVENTER_HOME=./converter
    echo "Start Converter"
    # For bool flags, = is necessary,
    # since --enable_bulkload will be parsed as --enable_bulkload=true,
    # and --no_enable_bulkload will be parsed as --enable_bulkload=false
    if [ "$use_debezium" == 1 ]; then
        echo "    Use Debezium converter"
        $CONVENTER_HOME/binlog_convert_debezium \
            --rg_mapping_file_path $rgmapping_file \
            --subgraph_num $subgraph_num --logs_per_epoch 10000 \
            --enable_bulkload=$enable_bulkload &
    else
        echo "    Use Maxwell converter"
        $CONVENTER_HOME/binlog_convert_maxwell --rg_mapping_file_path $rgmapping_file \
            --subgraph_num $subgraph_num --logs_per_epoch 10000 &
    fi
    pids["Converter"]=$!
    sleep 2
fi

####################### Writer (VEGITO) #######################

WRITER_HOME=./vegito

echo "Start Writer"

for ((i=$start_slot; i<$start_slot + $slot; i++)); do
    $WRITER_HOME/vegito --v6d_ipc_socket $v6d_sock \
    --etcd_endpoint $etcd_endpoint --meta_prefix $etcd_prefix \
    --kafka_broker_list $kafka_server \
    --schema_file_path $rgmapping_file \
    --table_schema_file_path $table_schema_file \
    --subgraph_num $subgraph_num --subgraph_id $i &
    pids["Vegito-$i"]=$!
done

echo "Process IDs:"
for key in "${!pids[@]}"; do
    echo "    $key: ${pids[$key]}"
done

./check_process.sh "$(declare -p pids)" &

sleep 2 # wait for output
